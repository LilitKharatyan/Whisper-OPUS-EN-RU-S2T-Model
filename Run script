!pip install torch
!pip install sentencepiece
!pip install transformers
!pip install soundfile
!pip install sacremoses
import os
import torch
import librosa
import soundfile as sf
import time
import psutil
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from google.colab import drive
drive.mount('/content/drive')
audio_folder = '/content/drive/My Drive/Ted'
chunk_duration = 30 #In seconds
def load_whisper_medium_model():
    processor = WhisperProcessor.from_pretrained("openai/whisper-medium")
    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-medium")
    model.config.forced_decoder_ids = None
    return processor, model

def load_opus_mt_en_ru_model():
    tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ru")
    model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-ru")
    return tokenizer, model
def transcribe_and_translate(audio_folder, chunk_duration):
    max_chunk_length = 3914 - 2

    all_files = os.listdir(audio_folder)
    audio_files = [filename for filename in all_files if filename.endswith((".wav", ".flac", ".mp3"))]

    asr_processor, asr_model = load_whisper_medium_model()
    mt_tokenizer, mt_model = load_opus_mt_en_ru_model()

    for audio_file in audio_files:

        # Get CPU usage and memory consumption before loading audio file
        cpu_percent_start = psutil.cpu_percent()
        memory_mb_start = psutil.virtual_memory().used / 1e6

        audio, sample_rate = sf.read(os.path.join(audio_folder, audio_file))

        audio_duration = len(audio) / sample_rate
        if audio_duration <= 40:
            start_time_transcribe = time.time()
            input_features = asr_processor(audio, sampling_rate=sample_rate, return_tensors="pt").input_features
            predicted_ids = asr_model.generate(input_features)
            transcription = asr_processor.batch_decode(predicted_ids, skip_special_tokens=True)
            transcription = transcription[0]
            transcribe_time = time.time() - start_time_transcribe

            start_time_translate = time.time()
            input_tokens = mt_tokenizer.encode(transcription, return_tensors="pt")
            output_tokens = mt_model.generate(input_tokens, max_new_tokens=4000, num_beams=28)
            translation = mt_tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            translate_time = time.time() - start_time_translate

            # Get CPU usage and memory consumption
            cpu_percent = psutil.cpu_percent()
            memory_mb = psutil.virtual_memory().used / 1e6


            print(f"Transcription of {audio_file}: {transcription}")
            print(f"Translation of {audio_file}: {translation}")
            print(f"CPU usage for audio file {audio_file}: {cpu_percent:.2f}%")
            print(f"Memory consumption for audio file {audio_file}: {memory_mb:.2f} MB")
            print(f"Transcription Time: {transcribe_time:.2f} seconds")
            print(f"Translation Time: {translate_time:.2f} seconds")
            print(f"Total Time: {transcribe_time + translate_time:.2f} seconds")
        else:
            num_chunks = int(len(audio) / sample_rate / chunk_duration) + 1
            chunks = [audio[i * sample_rate * chunk_duration: (i+1) * sample_rate * chunk_duration] for i in range(num_chunks)]

            transcription = ""
            for chunk in chunks:
                if sample_rate != 16000:
                    chunk = librosa.resample(chunk, sample_rate, 16000)

                inputs = asr_processor(chunk, sampling_rate=16000, return_tensors="pt", padding=True)

                with torch.no_grad():
                    logits = asr_model(input_values=inputs.input_values, attention_mask=inputs.attention_mask).logits
                predicted_ids = torch.argmax(logits, dim=-1)
                transcription_chunk = asr_processor.decode(predicted_ids[0])
                transcription += transcription_chunk.lower() + " "


            input_chunks = []
            for i in range(0, len(transcription), max_chunk_length):
                input_chunk = transcription[i:i + max_chunk_length]
                input_chunks.append(input_chunk)

            outputs = []
            for input_chunk in input_chunks:
                input_ids = mt_tokenizer.encode(input_chunk, return_tensors="pt")

                output = mt_model.generate(input_ids, max_new_tokens=4000, num_beams=28)
                outputs.append(output)

            decoded_outputs = [mt_tokenizer.decode(output[0], skip_special_tokens=True) for output in outputs]
            decoded_text = "".join(decoded_outputs)

            print(f"Transcription of {audio_file}: {transcription}")
            print(f"Translation of {audio_file}: {decoded_text}")
